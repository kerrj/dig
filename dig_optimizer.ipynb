{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell loads the model from the config file and initializes the viewer\n",
    "'''\n",
    "# %matplotlib widget\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from nerfstudio.utils.eval_utils import eval_setup\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from nerfstudio.viewer.viewer import Viewer\n",
    "from nerfstudio.configs.base_config import ViewerConfig\n",
    "import cv2\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "from typing import List,Optional,Literal\n",
    "from nerfstudio.utils import writer\n",
    "import time\n",
    "from threading import Lock\n",
    "import kornia\n",
    "from lerf.dig import DiGModel\n",
    "from lerf.data.utils.dino_dataloader import DinoDataloader\n",
    "from nerfstudio.cameras.cameras import Cameras\n",
    "from copy import deepcopy\n",
    "from torchvision.transforms.functional import resize\n",
    "from contextlib import nullcontext\n",
    "from lerf.zed import Zed\n",
    "from nerfstudio.engine.schedulers import ExponentialDecayScheduler,ExponentialDecaySchedulerConfig\n",
    "import warp as wp\n",
    "wp.init()\n",
    "\n",
    "# config = Path(\"outputs/nerfgun2/dig/2024-05-03_161203/config.yml\")\n",
    "# config = Path(\"outputs/nerfgun3/dig/2024-05-03_170424/config.yml\")\n",
    "# config = Path(\"outputs/nerfgun4/dig/2024-05-07_130351/config.yml\")\n",
    "# config = Path(\"outputs/painter_sculpture/dig/2024-05-10_132522/config.yml\")\n",
    "# config = Path(\"outputs/painter_sculpture/dig/2024-05-16_233028/config.yml\")#with ruilongs v2\n",
    "# config = Path(\"outputs/buddha_balls_poly/dig/2024-05-09_123412/config.yml\")\n",
    "# config = Path(\"outputs/buddha_balls_poly/dig/2024-05-16_231213/config.yml\")#with ruilongs v2\n",
    "# config = Path(\"outputs/cal_bear/dig/2024-05-15_155531/config.yml\")#this one groups table with bear for some reason\n",
    "# config = Path(\"outputs/boops_mug/dig/2024-05-10_223745/config.yml\")\n",
    "# config = Path(\"outputs/bww_faucet/dig/2024-05-12_215440/config.yml\")\n",
    "# config = Path(\"outputs/cmk_tpose2/dig/2024-05-14_142439/config.yml\")\n",
    "# config = Path(\"outputs/cal_bear/dig/2024-05-17_142920/config.yml\")#ruilong v2\n",
    "# config = Path(\"outputs/mac_charger/dig/2024-05-17_145312/config.yml\")\n",
    "# config = Path(\"outputs/mac_charger2/dig/2024-05-17_152545/config.yml\")\n",
    "# config = Path(\"outputs/glue_gun/dig/2024-05-17_161408/config.yml\")\n",
    "# config = Path(\"outputs/buddha_balls_poly/dig/2024-05-19_122050/config.yml\")# reuilong v2, 32-dim gauss\n",
    "# config = Path(\"outputs/mac_charger/dig/2024-05-19_125443/config.yml\")\n",
    "config = Path(\"outputs/mac_charger2/dig/2024-05-19_132100/config.yml\")\n",
    "OUTPUT_FOLDER = Path(\"renders/mac_charger\")\n",
    "\n",
    "assert OUTPUT_FOLDER.stem in str(config), \"Output folder name does not match config name\"\n",
    "OUTPUT_FOLDER.mkdir(exist_ok=True)\n",
    "train_config,pipeline,_,_ = eval_setup(config)\n",
    "dino_loader = pipeline.datamanager.dino_dataloader\n",
    "train_config.logging.local_writer.enable = False\n",
    "# We need to set up the writer to track number of rays, otherwise the viewer will not calculate the resolution correctly\n",
    "writer.setup_local_writer(train_config.logging, max_iter=train_config.max_num_iterations)\n",
    "v = Viewer(ViewerConfig(default_composite_depth=False,num_rays_per_chunk=-1),config.parent,pipeline.datamanager.get_datapath(),pipeline,train_lock=Lock())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n",
    "from typing import Union\n",
    "from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\n",
    "da_image_processor = AutoImageProcessor.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n",
    "da_model = AutoModelForDepthEstimation.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n",
    "da_model.to('cuda')\n",
    "def get_depth(img: Union[torch.tensor,np.ndarray]):\n",
    "    assert img.shape[2] == 3\n",
    "    if isinstance(img,torch.Tensor):\n",
    "        img = img.cpu().numpy()\n",
    "    image = Image.fromarray(img)\n",
    "\n",
    "    # prepare image for the model\n",
    "    inputs = da_image_processor(images=image, return_tensors=\"pt\")\n",
    "    inputs['pixel_values'] = inputs['pixel_values'].cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = da_model(**inputs)\n",
    "        predicted_depth = outputs.predicted_depth\n",
    "\n",
    "    # interpolate to original size\n",
    "    prediction = torch.nn.functional.interpolate(\n",
    "        predicted_depth.unsqueeze(1),\n",
    "        size=image.size[::-1],\n",
    "        mode=\"bicubic\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "    return prediction.squeeze()\n",
    "hand_processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-base-coco-panoptic\")\n",
    "hand_model = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-base-coco-panoptic\")\n",
    "hand_model.to('cuda')\n",
    "def get_hand_mask(img: Union[torch.tensor,np.ndarray]):\n",
    "    assert img.shape[2] == 3\n",
    "    if isinstance(img,torch.Tensor):\n",
    "        img = img.cpu().numpy()\n",
    "    image = Image.fromarray(img)\n",
    "\n",
    "    # prepare image for the model\n",
    "    inputs = hand_processor(images=image, return_tensors=\"pt\")\n",
    "    inputs['pixel_values'] = inputs['pixel_values'].cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = hand_model(**inputs)\n",
    "\n",
    "    # Perform post-processing to get panoptic segmentation map\n",
    "    seg_ids = hand_processor.post_process_semantic_segmentation(\n",
    "        outputs, target_sizes=[image.size[::-1]]\n",
    "    )[0]\n",
    "    hand_mask = (seg_ids == hand_model.config.label2id['person']).float()\n",
    "    return hand_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell defines a simple pose optimizer for learning a rigid transform offset given a gaussian model, star pose, and starting view\n",
    "\"\"\"\n",
    "\n",
    "def get_vid_frame(cap,timestamp):\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Calculate the frame number based on the timestamp and fps\n",
    "    frame_number = min(int(timestamp * fps),int(cap.get(cv2.CAP_PROP_FRAME_COUNT)-1))\n",
    "    \n",
    "    # Set the video position to the calculated frame number\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    \n",
    "    # Read the frame\n",
    "    success, frame = cap.read()\n",
    "    # convert BGR to RGB\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    return frame\n",
    "        \n",
    "def quatmul(q0:torch.Tensor,q1:torch.Tensor):\n",
    "    w0, x0, y0, z0 = torch.unbind(q0, dim=-1)\n",
    "    w1, x1, y1, z1 = torch.unbind(q1, dim=-1)\n",
    "    return torch.stack(\n",
    "            [\n",
    "                -x0 * x1 - y0 * y1 - z0 * z1 + w0 * w1,\n",
    "                x0 * w1 + y0 * z1 - z0 * y1 + w0 * x1,\n",
    "                -x0 * z1 + y0 * w1 + z0 * x1 + w0 * y1,\n",
    "                x0 * y1 - y0 * x1 + z0 * w1 + w0 * z1,\n",
    "            ],\n",
    "            dim = -1\n",
    "        )\n",
    "\n",
    "def depth_ranking_loss(rendered_depth, gt_depth):\n",
    "    \"\"\"\n",
    "    Depth ranking loss as described in the SparseNeRF paper\n",
    "    Assumes that the layout of the batch comes from a PairPixelSampler, so that adjacent samples in the gt_depth\n",
    "    and rendered_depth are from pixels with a radius of each other\n",
    "    \"\"\"\n",
    "    m = 1e-4\n",
    "    if rendered_depth.shape[0] % 2 != 0:\n",
    "        # chop off one index\n",
    "        rendered_depth = rendered_depth[:-1, :]\n",
    "        gt_depth = gt_depth[:-1, :]\n",
    "    dpt_diff = gt_depth[::2, :] - gt_depth[1::2, :]\n",
    "    out_diff = rendered_depth[::2, :] - rendered_depth[1::2, :] + m\n",
    "    differing_signs = torch.sign(dpt_diff) != torch.sign(out_diff)\n",
    "    loss = (out_diff[differing_signs] * torch.sign(out_diff[differing_signs]))\n",
    "    med = loss.quantile(.8)\n",
    "    return loss[loss < med].mean()\n",
    "\n",
    "@wp.kernel\n",
    "def apply_to_model(pose_deltas: wp.array(dtype = float, ndim = 2), means: wp.array(dtype = wp.vec3), quats: wp.array(dtype = float,ndim=2),\n",
    "                    group_labels: wp.array(dtype = int), centroids: wp.array(dtype = wp.vec3),\n",
    "                    means_out: wp.array(dtype = wp.vec3), quats_out: wp.array(dtype = float,ndim=2)):\n",
    "    \"\"\"\n",
    "    Takes the current pose_deltas and applies them to each of the group masks\n",
    "    \"\"\"\n",
    "    tid = wp.tid()\n",
    "    group_id = group_labels[tid]\n",
    "    position = wp.vector(pose_deltas[group_id,0],pose_deltas[group_id,1],pose_deltas[group_id,2])\n",
    "    #pose_deltas are in w x y z, we need to flip\n",
    "    quaternion = wp.quaternion(pose_deltas[group_id,4],pose_deltas[group_id,5],pose_deltas[group_id,6],pose_deltas[group_id,3])\n",
    "    transform = wp.transformation(position,quaternion)\n",
    "    means_out[tid] = wp.transform_point(transform,means[tid] - centroids[tid]) + centroids[tid]\n",
    "    gauss_quaternion = wp.quaternion(quats[tid,1],quats[tid,2],quats[tid,3],quats[tid,0])\n",
    "    newquat = quaternion*gauss_quaternion\n",
    "    quats_out[tid,0] = newquat[3]\n",
    "    quats_out[tid,1] = newquat[0]\n",
    "    quats_out[tid,2] = newquat[1]\n",
    "    quats_out[tid,3] = newquat[2]\n",
    "    \n",
    "@wp.kernel\n",
    "def atap_loss(cur_means: wp.array(dtype = wp.vec3), dists: wp.array(dtype = float), ids: wp.array(dtype = int),\n",
    "               match_ids: wp.array(dtype = int), group_ids1: wp.array(dtype = int), group_ids2: wp.array(dtype=int), \n",
    "               connectivity_weights: wp.array(dtype = float,ndim = 2), loss: wp.array(dtype = float)):\n",
    "    tid = wp.tid()\n",
    "    id1 = ids[tid]\n",
    "    id2 = match_ids[tid]\n",
    "    gid1 = group_ids1[tid]\n",
    "    gid2 = group_ids2[tid]\n",
    "    con_weight = connectivity_weights[gid1,gid2]\n",
    "    curdist = wp.length(cur_means[id1] - cur_means[id2])\n",
    "    loss[tid] = wp.abs(curdist - dists[tid]) * con_weight\n",
    "\n",
    "class ATAPLoss:\n",
    "    touch_radius: float = .01\n",
    "    N: int = 10\n",
    "    loss_mult: float = .05\n",
    "    def __init__(self, dig_model: DiGModel, group_masks: List[torch.Tensor], group_labels: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Initializes the data structure to compute the loss between groups touching\n",
    "        \"\"\"\n",
    "        self.dig_model = dig_model\n",
    "        self.group_masks = group_masks\n",
    "        self.group_labels = group_labels\n",
    "        self.nn_info = []\n",
    "        for grp in self.group_masks:\n",
    "            with torch.no_grad():\n",
    "                dists, ids, match_ids, group_ids1, group_ids = self._radius_nn(grp, self.touch_radius)\n",
    "                self.nn_info.append((dists, ids, match_ids, group_ids1, group_ids))\n",
    "                print(f\"Group {len(self.nn_info)} has {len(ids)} neighbors\")\n",
    "        self.dists = torch.cat([x[0] for x in self.nn_info]).cuda()\n",
    "        self.ids = torch.cat([x[1] for x in self.nn_info]).cuda().int()\n",
    "        self.match_ids = torch.cat([x[2] for x in self.nn_info]).cuda().int()\n",
    "        self.group_ids1 = torch.cat([x[3] for x in self.nn_info]).cuda().int()\n",
    "        self.group_ids2 = torch.cat([x[4] for x in self.nn_info]).cuda().int()\n",
    "        self.num_pairs = torch.cat([torch.tensor(len(x[1])).repeat(len(x[1])) for x in self.nn_info]).cuda().float()\n",
    "        \n",
    "\n",
    "    def __call__(self, connectivity_weights: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Computes the loss between groups touching\n",
    "        connectivity_weights: a tensor of shape (num_groups,num_groups) representing the weights between each group\n",
    "\n",
    "        returns: a differentiable loss\n",
    "        \"\"\"\n",
    "        assert connectivity_weights.shape == (len(self.group_masks),len(self.group_masks)), \"connectivity weights must be a square matrix of size num_groups\"\n",
    "        loss = wp.empty(self.dists.shape[0], dtype=wp.float32, requires_grad=True)\n",
    "        wp.launch(\n",
    "            dim = self.dists.shape[0],\n",
    "            kernel = atap_loss,\n",
    "            inputs = [wp.from_torch(self.dig_model.gauss_params['means'],dtype=wp.vec3),wp.from_torch(self.dists),\n",
    "                      wp.from_torch(self.ids),wp.from_torch(self.match_ids),wp.from_torch(self.group_ids1),\n",
    "                      wp.from_torch(self.group_ids2),wp.from_torch(connectivity_weights),loss]\n",
    "        )\n",
    "        return (wp.to_torch(loss)/self.num_pairs).sum()*self.loss_mult\n",
    "        \n",
    "\n",
    "    def _radius_nn(self, group_mask: torch.Tensor, r: float):\n",
    "        \"\"\"\n",
    "        returns the nearest neighbors to gaussians in a group within a certain radius (and outside that group)\n",
    "        returns -1 indices for neighbors outside the radius or within the same group\n",
    "        \"\"\"\n",
    "        global_group_ids = torch.zeros(self.dig_model.num_points,dtype=torch.long,device='cuda')\n",
    "        for i,grp in enumerate(self.group_masks):\n",
    "            global_group_ids[grp] = i\n",
    "        from cuml.neighbors import NearestNeighbors\n",
    "        model = NearestNeighbors(n_neighbors=self.N)\n",
    "        means = self.dig_model.means.detach().cpu().numpy()\n",
    "        model.fit(means)\n",
    "        dists, match_ids = model.kneighbors(means)\n",
    "        dists, match_ids = torch.tensor(dists,dtype=torch.float32,device='cuda'),torch.tensor(match_ids,dtype=torch.long,device='cuda')\n",
    "        dists, match_ids = dists[group_mask], match_ids[group_mask]\n",
    "        # filter matches outside the radius\n",
    "        match_ids[dists>r] = -1\n",
    "        # filter out ones within same group mask\n",
    "        match_ids[group_mask[match_ids]] = -1\n",
    "        ids = torch.arange(self.dig_model.num_points,dtype=torch.long,device='cuda')[group_mask].unsqueeze(-1).repeat(1,self.N)\n",
    "        #flatten all the ids/dists/match_ids\n",
    "        ids = ids[match_ids!=-1].flatten()\n",
    "        dists = dists[match_ids!=-1].flatten()\n",
    "        match_ids = match_ids[match_ids!=-1].flatten()\n",
    "        return dists, ids, match_ids, global_group_ids[ids], global_group_ids[match_ids]\n",
    "\n",
    "try:\n",
    "    loss_plt.remove()\n",
    "except:\n",
    "    pass\n",
    "class RigidGroupOptimizer:\n",
    "    use_depth: bool = True\n",
    "    depth_ignore_threshold: float = 0.02 # in meters\n",
    "    use_atap: bool = True\n",
    "    pose_lr: float = .005\n",
    "    pose_lr_final: float = .0005\n",
    "    mask_hands: bool = False\n",
    "    def __init__(self, dig_model: DiGModel, dino_loader: DinoDataloader, init_c2o: Cameras, group_masks: List[torch.Tensor], group_labels: torch.Tensor, render_lock = nullcontext()):\n",
    "        \"\"\"\n",
    "        This one takes in a list of gaussian ID masks to optimize local poses for\n",
    "        Each rigid group can be optimized independently, with no skeletal constraints\n",
    "        \"\"\"\n",
    "        self.tape = None\n",
    "        self.dig_model = dig_model\n",
    "        #detach all the params to avoid retain_graph issue\n",
    "        self.dig_model.gauss_params['means'] = self.dig_model.gauss_params['means'].detach()\n",
    "        self.dig_model.gauss_params['quats'] = self.dig_model.gauss_params['quats'].detach()\n",
    "        self.dino_loader = dino_loader\n",
    "        self.group_labels = group_labels\n",
    "        self.group_masks = group_masks\n",
    "        self.init_c2o = deepcopy(init_c2o).to('cuda')\n",
    "        #store a 7-vec of trans, rotation for each group\n",
    "        self.pose_deltas = torch.zeros(len(group_masks),7,dtype=torch.float32,device='cuda')\n",
    "        self.pose_deltas[:,3:] = torch.tensor([1,0,0,0],dtype=torch.float32,device='cuda')\n",
    "        self.pose_deltas = torch.nn.Parameter(self.pose_deltas)\n",
    "        k = 3\n",
    "        s = 0.3 * ((k - 1) * 0.5 - 1) + 0.8\n",
    "        self.blur = kornia.filters.GaussianBlur2d((k, k), (s, s))\n",
    "        #NOT USED RN\n",
    "        self.connectivity_weights = torch.nn.Parameter(-torch.ones(len(group_masks),len(group_masks),dtype=torch.float32,device='cuda'))\n",
    "        self.optimizer = torch.optim.Adam([self.pose_deltas],lr=self.pose_lr)\n",
    "        # self.weights_optimizer = torch.optim.Adam([self.connectivity_weights],lr=.001)\n",
    "        self.init_means = dig_model.gauss_params['means'].detach().clone()\n",
    "        self.init_quats = dig_model.gauss_params['quats'].detach().clone()\n",
    "        self.keyframes = []\n",
    "        # lock to prevent blocking the render thread if provided\n",
    "        self.render_lock = render_lock\n",
    "        if self.use_atap:\n",
    "            self.atap = ATAPLoss(dig_model,group_masks,group_labels)\n",
    "        self.centroids = torch.empty((self.dig_model.num_points,3),dtype=torch.float32,device='cuda',requires_grad=False)\n",
    "        for i,mask in enumerate(self.group_masks):\n",
    "            with torch.no_grad():\n",
    "                self.centroids[mask] = self.dig_model.gauss_params['means'][mask].mean(dim=0)\n",
    "\n",
    "    def step(self, niter = 1, use_depth = True, use_rgb = False, metric_depth = False):\n",
    "        scheduler = ExponentialDecayScheduler(ExponentialDecaySchedulerConfig(lr_final = self.pose_lr_final, max_steps=niter)).get_scheduler(self.optimizer, self.pose_lr)\n",
    "        for i in range(niter):\n",
    "            # renormalize rotation representation\n",
    "            with torch.no_grad():\n",
    "                self.pose_deltas[:,3:] = self.pose_deltas[:,3:]/self.pose_deltas[:,3:].norm(dim=1,keepdim=True)\n",
    "            tape = wp.Tape()\n",
    "            self.optimizer.zero_grad()\n",
    "            # self.weights_optimizer.zero_grad()\n",
    "            with self.render_lock:\n",
    "                self.dig_model.eval()\n",
    "                with tape:\n",
    "                    self.apply_to_model(self.pose_deltas)\n",
    "                dig_outputs = self.dig_model.get_outputs(self.init_c2o)\n",
    "            if 'dino' not in dig_outputs:\n",
    "                self.reset_transforms()\n",
    "                raise RuntimeError(\"Lost tracking\")\n",
    "            with torch.no_grad():\n",
    "                object_mask = dig_outputs['accumulation']>.9\n",
    "            dino_feats = self.blur(dig_outputs[\"dino\"].permute(2,0,1)[None]).squeeze().permute(1,2,0)\n",
    "            if self.mask_hands:\n",
    "                pix_loss = (self.frame_pca_feats - dino_feats)[self.hand_mask]\n",
    "            else:\n",
    "                pix_loss = (self.frame_pca_feats - dino_feats)\n",
    "            # THIS IS BAD WE NEED TO FIX THIS (because resizing makes the image very slightly misaligned)\n",
    "            loss = pix_loss.norm(dim=-1).mean()\n",
    "            if use_depth and self.use_depth:\n",
    "                if metric_depth:\n",
    "                    physical_depth = dig_outputs['depth']/pipeline.datamanager.train_dataset._dataparser_outputs.dataparser_scale\n",
    "                    valids = object_mask & (~self.frame_depth.isnan())\n",
    "                    if self.mask_hands:\n",
    "                        valids = valids & self.hand_mask.unsqueeze(-1)\n",
    "                    pix_loss = (physical_depth - self.frame_depth)**2\n",
    "                    pix_loss = pix_loss[valids & (pix_loss<self.depth_ignore_threshold**2)]\n",
    "                    loss = loss + 0.1*pix_loss.mean()\n",
    "                else:\n",
    "                    # This is ranking loss for monodepth (which is disparity)\n",
    "                    disparity = 1.0 / dig_outputs['depth']\n",
    "                    N = 20000\n",
    "                    if self.mask_hands:\n",
    "                        object_mask = object_mask & self.hand_mask.unsqueeze(-1)\n",
    "                    valid_ids = torch.where(object_mask)\n",
    "                    rand_samples = torch.randint(0,valid_ids[0].shape[0],(N,),device='cuda')\n",
    "                    rand_samples = (valid_ids[0][rand_samples],valid_ids[1][rand_samples])\n",
    "                    rend_samples = disparity[rand_samples]\n",
    "                    mono_samples = self.frame_depth[rand_samples]\n",
    "                    rank_loss = depth_ranking_loss(rend_samples,mono_samples)\n",
    "                    loss = loss + 0.5*rank_loss\n",
    "            if use_rgb:\n",
    "                loss = loss + .05*(dig_outputs['rgb']-self.rgb_frame).abs().mean()\n",
    "            if self.use_atap:\n",
    "                null_weights = torch.ones_like(self.connectivity_weights)\n",
    "                # null_weights = self.connectivity_weights.exp()\n",
    "                weights = torch.clip(null_weights,0,1)\n",
    "                with tape:\n",
    "                    atap_loss = self.atap(weights)\n",
    "                rigidity_loss = .02*(1-weights).mean()\n",
    "                symmetric_loss = (weights - weights.T).abs().mean()\n",
    "                #maximize the connectivity weights, as well as similarity\n",
    "                loss = loss + atap_loss + symmetric_loss + rigidity_loss\n",
    "            loss.backward()\n",
    "            tape.backward()\n",
    "            self.optimizer.step()\n",
    "            # self.weights_optimizer.step()\n",
    "            scheduler.step()\n",
    "        #reset lr\n",
    "        self.optimizer.param_groups[0]['lr'] = self.pose_lr\n",
    "        return dig_outputs\n",
    "    \n",
    "    def apply_to_model(self,pose_deltas):\n",
    "        \"\"\"\n",
    "        Takes the current pose_deltas and applies them to each of the group masks\n",
    "        \"\"\"\n",
    "        self.reset_transforms()\n",
    "        new_quats = torch.empty_like(self.dig_model.gauss_params['quats'],requires_grad=False)\n",
    "        new_means = torch.empty_like(self.dig_model.gauss_params['means'],requires_grad=True)\n",
    "        wp.launch(\n",
    "            kernel = apply_to_model,\n",
    "            dim = self.dig_model.num_points,\n",
    "            inputs = [wp.from_torch(pose_deltas),wp.from_torch(self.dig_model.gauss_params['means'],dtype=wp.vec3),\n",
    "                    wp.from_torch(self.dig_model.gauss_params['quats']),wp.from_torch(self.group_labels),\n",
    "                    wp.from_torch(self.centroids,dtype=wp.vec3)],\n",
    "            outputs = [wp.from_torch(new_means,dtype=wp.vec3),wp.from_torch(new_quats)]\n",
    "        )\n",
    "        self.dig_model.gauss_params['quats'] = new_quats\n",
    "        self.dig_model.gauss_params['means'] = new_means\n",
    "\n",
    "\n",
    "    def register_keyframe(self):\n",
    "        \"\"\"\n",
    "        Saves the current pose_deltas as a keyframe\n",
    "        \"\"\"\n",
    "        self.keyframes.append(self.pose_deltas.detach().clone())\n",
    "\n",
    "    def apply_keyframe(self,i):\n",
    "        \"\"\"\n",
    "        Applies the ith keyframe to the pose_deltas\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.apply_to_model(self.keyframes[i])\n",
    "\n",
    "    def reset_transforms(self):\n",
    "        with torch.no_grad():\n",
    "            self.dig_model.gauss_params['means'] = self.init_means.clone()\n",
    "            self.dig_model.gauss_params['quats'] = self.init_quats.clone()\n",
    "\n",
    "    def set_frame(self, rgb_frame: torch.Tensor, depth: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Sets the rgb_frame to optimize the pose for\n",
    "        rgb_frame: HxWxC tensor image\n",
    "        init_c2o: initial camera to object transform (given whatever coordinates the self.dig_model is in)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.rgb_frame = resize(rgb_frame.permute(2,0,1), (self.init_c2o.height,self.init_c2o.width),antialias = True).permute(1,2,0)\n",
    "            self.frame_pca_feats = self.dino_loader.get_pca_feats(rgb_frame.permute(2,0,1).unsqueeze(0),keep_cuda=True).squeeze()\n",
    "            self.frame_pca_feats = resize(self.frame_pca_feats.permute(2,0,1), (self.init_c2o.height,self.init_c2o.width),antialias = True).permute(1,2,0)\n",
    "            if self.use_depth:\n",
    "                if depth is None:\n",
    "                    depth = get_depth((self.rgb_frame*255).to(torch.uint8))\n",
    "                self.frame_depth = resize(depth.unsqueeze(0), (self.init_c2o.height,self.init_c2o.width),antialias = True).squeeze().unsqueeze(-1)\n",
    "            if self.mask_hands:\n",
    "                self.hand_mask = get_hand_mask((self.rgb_frame*255).to(torch.uint8))\n",
    "                self.hand_mask = torch.nn.functional.max_pool2d(self.hand_mask[None,None],3,padding=1,stride=1).squeeze() == 0.0\n",
    "\n",
    "\n",
    "MATCH_RESOLUTION = 500\n",
    "camera_input = 'iphone' # ['train_cam', 'iphone','zed', 'iphone_vertical','zed_svo']\n",
    "video_path = Path(\"motion_vids/mac_charger_fold3.MOV\")\n",
    "svo_path = Path(\"motion_vids/buddha_close_good.svo2\")\n",
    "start_time = 0.3\n",
    "\n",
    "\n",
    "if camera_input == 'train_cam':\n",
    "    init_cam,data = pipeline.datamanager.next_train(0)\n",
    "    view_cam_pose = pipeline.viewer_control.get_camera(200,None,0)\n",
    "    init_cam.camera_to_worlds = view_cam_pose.camera_to_worlds\n",
    "    init_cam.rescale_output_resolution(MATCH_RESOLUTION/max(init_cam.width,init_cam.height))\n",
    "elif camera_input == 'iphone':\n",
    "    init_cam = Cameras(camera_to_worlds=pipeline.viewer_control.get_camera(200,None,0).camera_to_worlds,fx = 1137.0,fy = 1137.0,cx = 1280.0/2,cy = 720/2,width=1280,height=720)\n",
    "    init_cam.rescale_output_resolution(MATCH_RESOLUTION/max(init_cam.width,init_cam.height))\n",
    "elif camera_input == 'iphone_vertical':\n",
    "    init_cam = Cameras(camera_to_worlds=pipeline.viewer_control.get_camera(200,None,0).camera_to_worlds,fy = 1137.0,fx = 1137.0,cy = 1280/2,cx = 720/2,height=1280,width=720)\n",
    "    init_cam.rescale_output_resolution(MATCH_RESOLUTION/max(init_cam.width,init_cam.height))\n",
    "elif camera_input in ['zed','zed_svo']:\n",
    "    try:\n",
    "        zed.cam.close()\n",
    "        del zed\n",
    "    except:\n",
    "        pass\n",
    "    finally:\n",
    "        zed = Zed(recording_file=str(svo_path.absolute()) if camera_input == 'zed_svo' else None, start_time=start_time)\n",
    "    fps = 30\n",
    "    left_rgb,_,_ = zed.get_frame()\n",
    "    K = zed.get_K()\n",
    "    init_cam = Cameras(camera_to_worlds=pipeline.viewer_control.get_camera(200,None,0).camera_to_worlds,fx = K[0,0],fy = K[1,1],cx = K[0,2],cy = K[1,2],width=1920,height=1080)\n",
    "    init_cam.rescale_output_resolution(MATCH_RESOLUTION/max(init_cam.width,init_cam.height))\n",
    "outputs = pipeline.model.get_outputs_for_camera(init_cam)\n",
    "if pipeline.cluster_labels is not None:\n",
    "    labels = pipeline.cluster_labels.int().cuda()\n",
    "    group_masks = [(cid == labels).cuda() for cid in range(labels.max() + 1)]\n",
    "else:\n",
    "    labels = torch.zeros(pipeline.model.num_points).int().cuda()\n",
    "    group_masks = [torch.ones(pipeline.model.num_points).bool().cuda()]\n",
    "optimizer = RigidGroupOptimizer(pipeline.model,dino_loader,init_cam,group_masks, group_labels = labels, render_lock = v.train_lock)\n",
    "rgb_renders = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if camera_input in ['zed','zed_svo']:\n",
    "    left_rgb, right_rgb,depth = zed.get_frame()\n",
    "    target_frame_rgb = (left_rgb/255)\n",
    "    right_frame_rgb = (right_rgb/255)\n",
    "    optimizer.set_frame(target_frame_rgb,depth=depth)\n",
    "else:\n",
    "    assert video_path.exists()\n",
    "    motion_clip = cv2.VideoCapture(str(video_path.absolute()))\n",
    "    start=1\n",
    "    end=4\n",
    "    fps = 30\n",
    "    frame = get_vid_frame(motion_clip,start)\n",
    "    target_frame_rgb = ToTensor()(Image.fromarray(frame)).permute(1,2,0).cuda()\n",
    "    optimizer.set_frame(target_frame_rgb)\n",
    "_,axs = plt.subplots(1,2,figsize=(10,4))\n",
    "axs[0].imshow(outputs[\"rgb\"].detach().cpu().numpy())\n",
    "axs[1].imshow(target_frame_rgb.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerfstudio.utils.colormaps import apply_depth_colormap\n",
    "import tqdm\n",
    "import moviepy.editor as mpy\n",
    "import plotly.express as px\n",
    "def plotly_render(frame):\n",
    "    fig = px.imshow(frame)\n",
    "    fig.update_layout(\n",
    "        margin=dict(l=0, r=0, t=0, b=0),showlegend=False,yaxis_visible=False, yaxis_showticklabels=False,xaxis_visible=False, xaxis_showticklabels=False\n",
    "    )\n",
    "    return fig\n",
    "fig = plotly_render(outputs['rgb'].detach().cpu().numpy())\n",
    "try:\n",
    "    frame_vis.remove()\n",
    "except:\n",
    "    pass\n",
    "frame_vis = pipeline.viewer_control.viser_server.add_gui_plotly(fig, 9/16)\n",
    "try:\n",
    "    animate_button.remove()\n",
    "    frame_slider.remove()\n",
    "    reset_button.remove()\n",
    "except:\n",
    "    pass\n",
    "def composite_vis_frame(target_frame_rgb,outputs):\n",
    "    target_vis_frame = resize(target_frame_rgb.permute(2,0,1),(outputs[\"rgb\"].shape[0],outputs[\"rgb\"].shape[1])).permute(1,2,0)\n",
    "    # composite the outputs['rgb'] on top of target_vis frame\n",
    "    target_vis_frame = target_vis_frame*0.5 + outputs[\"rgb\"]*0.5\n",
    "    return target_vis_frame\n",
    "\n",
    "try:\n",
    "    render_button.remove()\n",
    "    filename_input.remove()\n",
    "    status_mkdown.remove()\n",
    "except:\n",
    "    pass\n",
    "import viser\n",
    "filename_input = v.viser_server.add_gui_text(\"File Name\",\"render\")\n",
    "status_mkdown = v.viser_server.add_gui_markdown(\" \")\n",
    "render_button = v.viser_server.add_gui_button(\"Render Animation\",color='green',icon=viser.Icon.MOVIE)\n",
    "@render_button.on_click\n",
    "def render(_):\n",
    "    render_button.disabled = True\n",
    "    render_frames = []\n",
    "    camera = pipeline.viewer_control.get_camera(1080,1920,0)\n",
    "    for i in tqdm.tqdm(range(len(optimizer.keyframes))):\n",
    "        status_mkdown.content = f\"Rendering...{i/len(optimizer.keyframes):.01f}\"\n",
    "        pipeline.model.eval()\n",
    "        optimizer.apply_keyframe(i)\n",
    "        with torch.no_grad():\n",
    "            outputs = pipeline.model.get_outputs_for_camera(camera)\n",
    "        render_frames.append(outputs[\"rgb\"].detach().cpu().numpy()*255)\n",
    "    status_mkdown.content = \"Saving...\"\n",
    "    out_clip = mpy.ImageSequenceClip(render_frames, fps=fps)\n",
    "    fname = filename_input.value\n",
    "    (OUTPUT_FOLDER / 'posed_renders').mkdir(exist_ok=True)\n",
    "    render_folder = OUTPUT_FOLDER / 'posed_renders'\n",
    "    out_clip.write_videofile(f\"{render_folder}/{fname}.mp4\", fps=fps,codec='libx264')\n",
    "    out_clip.write_videofile(f\"{render_folder}/{fname}_mac.mp4\", fps=fps,codec='mpeg4',bitrate='5000k')\n",
    "    v.viser_server.send_file_download(f\"{fname}_mac.mp4\",open(f\"{render_folder}/{fname}_mac.mp4\",'rb').read())\n",
    "    status_mkdown.content = \"Done!\"\n",
    "    render_button.disabled = False\n",
    "\n",
    "\n",
    "if camera_input in ['zed','zed_svo']:\n",
    "    if len(rgb_renders)==0:\n",
    "        for i in tqdm.tqdm(range(10)):\n",
    "            target_vis_frame = composite_vis_frame(target_frame_rgb,outputs)\n",
    "            vis_frame = torch.concatenate([outputs[\"rgb\"],target_vis_frame],dim=1).detach().cpu().numpy()\n",
    "            fig = plotly_render(target_vis_frame.detach().cpu().numpy())\n",
    "            frame_vis.figure = fig\n",
    "            rgb_renders.append(vis_frame*255)\n",
    "            outputs = optimizer.step(50, use_depth=i>7, metric_depth=True)\n",
    "    while True:\n",
    "        # If input camera is the zed, just loop it indefinitely until no more frames\n",
    "        left_rgb, _, depth = zed.get_frame()\n",
    "        if left_rgb is None:\n",
    "            break\n",
    "        target_frame_rgb = left_rgb/255\n",
    "        optimizer.set_frame(target_frame_rgb,depth=depth)\n",
    "        outputs = optimizer.step(50, metric_depth=True)\n",
    "        v._trigger_rerender()\n",
    "        optimizer.register_keyframe()\n",
    "        target_vis_frame = composite_vis_frame(target_frame_rgb,outputs)\n",
    "        vis_frame = torch.concatenate([outputs[\"rgb\"],target_vis_frame],dim=1).detach().cpu().numpy()\n",
    "        rgb_renders.append(vis_frame*255)\n",
    "        fig = plotly_render(target_vis_frame.detach().cpu().numpy())\n",
    "        frame_vis.figure = fig\n",
    "elif camera_input in ['iphone','iphone_vertical','train_cam']:\n",
    "    # Otherwise procces the video\n",
    "    if len(rgb_renders)==0:\n",
    "        for i in tqdm.tqdm(range(10)):\n",
    "            target_vis_frame = composite_vis_frame(target_frame_rgb,outputs)\n",
    "            vis_frame = torch.concatenate([outputs[\"rgb\"],target_vis_frame],dim=1).detach().cpu().numpy()\n",
    "            fig = plotly_render(target_vis_frame.detach().cpu().numpy())\n",
    "            frame_vis.figure = fig\n",
    "            rgb_renders.append(vis_frame*255)\n",
    "            outputs = optimizer.step(30, use_depth=i>7, metric_depth=False)\n",
    "\n",
    "    for t in tqdm.tqdm(np.linspace(start,end,int((end-start)*fps))):\n",
    "        frame = get_vid_frame(motion_clip,t)\n",
    "        target_frame_rgb = ToTensor()(Image.fromarray(frame)).permute(1,2,0).cuda()\n",
    "        optimizer.set_frame(target_frame_rgb)\n",
    "        outputs = optimizer.step(50, metric_depth=False)\n",
    "        optimizer.register_keyframe()\n",
    "        v._trigger_rerender()\n",
    "        target_vis_frame = composite_vis_frame(target_frame_rgb,outputs)\n",
    "        vis_frame = torch.concatenate([outputs[\"rgb\"],target_vis_frame],dim=1).detach().cpu().numpy()\n",
    "        fig = plotly_render(target_vis_frame.detach().cpu().numpy())\n",
    "        frame_vis.figure = fig\n",
    "        rgb_renders.append(vis_frame*255)\n",
    "#save as an mp4\n",
    "out_clip = mpy.ImageSequenceClip(rgb_renders, fps=fps)  \n",
    "\n",
    "fname = str(OUTPUT_FOLDER / \"optimizer_out_dance.mp4\")\n",
    "\n",
    "out_clip.write_videofile(fname, fps=fps,codec='libx264')\n",
    "out_clip.write_videofile(fname.replace('.mp4','_mac.mp4'),fps=fps,codec='mpeg4',bitrate='5000k')\n",
    "\n",
    "# Populate some viewer elements to visualize the animation\n",
    "animate_button = v.viser_server.add_gui_button(\"Play Animation\")\n",
    "frame_slider = v.viser_server.add_gui_slider(\"Frame\",0,len(optimizer.keyframes)-1,1,0)\n",
    "reset_button = v.viser_server.add_gui_button(\"Reset Transforms\")\n",
    "\n",
    "@animate_button.on_click\n",
    "def play_animation(_):\n",
    "    for i in range(len(optimizer.keyframes)):\n",
    "        optimizer.apply_keyframe(i)\n",
    "        v._trigger_rerender()\n",
    "        time.sleep(1/fps)\n",
    "@frame_slider.on_update\n",
    "def apply_keyframe(_):\n",
    "    optimizer.apply_keyframe(frame_slider.value)\n",
    "    v._trigger_rerender()\n",
    "@reset_button.on_click\n",
    "def reset_transforms(_):\n",
    "    optimizer.reset_transforms()\n",
    "    v._trigger_rerender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "please",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
